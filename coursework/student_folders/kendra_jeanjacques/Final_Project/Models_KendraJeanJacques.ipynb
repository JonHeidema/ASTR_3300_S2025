{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a299d244-2888-46b0-b51a-76b75f05a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"fifa_players.csv\")\n",
    "\n",
    "# Drop irrelevant or non-numeric columns\n",
    "irrelevant_columns = [\n",
    "    'name', 'full_name', 'birth_date', 'positions', 'nationality',\n",
    "    'preferred_foot', 'body_type', 'national_team', 'national_team_position',\n",
    "    'national_jersey_number'\n",
    "]\n",
    "df_clean = df.drop(columns=irrelevant_columns)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "# Define features and target\n",
    "X = df_clean.drop(columns=['overall_rating'])\n",
    "y = df_clean['overall_rating']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'KNN': KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "# Train models, evaluate, and generate scatter plots\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Store evaluation metrics\n",
    "    results[name] = {\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'MSE': mean_squared_error(y_test, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'R2': r2_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "    # Generate scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=y_test, y=y_pred, alpha=0.6, s=50) # Increased marker size (s)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect Prediction')\n",
    "    plt.xlabel(\"Actual Overall Rating\")\n",
    "    plt.ylabel(\"Predicted Overall Rating\")\n",
    "    plt.title(f\"Actual vs Predicted Ratings for {name} Model\")  \n",
    "    plt.grid(True)\n",
    "    plt.legend() # Added legend\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"actual_vs_predicted_{name.lower().replace(' ', '_')}.png\", format=\"png\")\n",
    "    plt.close()\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results).T.sort_values(by='R2', ascending=False)\n",
    "print(\"Model Performance Comparison:\\n\")\n",
    "print(results_df)\n",
    "\n",
    "# Save model performance table as PNG (changed from PDF)\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.axis('off')\n",
    "table = plt.table(\n",
    "    cellText=np.round(results_df.values, 4),\n",
    "    colLabels=results_df.columns,\n",
    "    rowLabels=results_df.index,\n",
    "    cellLoc='center',\n",
    "    loc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "plt.title(\"Model Performance Comparison\", fontsize=14, weight='bold')\n",
    "plt.savefig(\"model_performance_comparison.png\", format=\"png\")\n",
    "plt.close()\n",
    "\n",
    "# Prepare data for grouped bar graph\n",
    "models_list = results_df.index\n",
    "metrics = ['RMSE', 'MSE', 'R2']\n",
    "bar_width = 0.25  # Width of each bar\n",
    "x = np.arange(len(models_list))  # X positions for each model\n",
    "\n",
    "# Create grouped bar graph - PLOTTING ORIGINAL VALUES\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars_rmse = plt.bar(x - bar_width, results_df['RMSE'], bar_width, label='RMSE', color='#1f77b4')\n",
    "bars_mse = plt.bar(x, results_df['MSE'], bar_width, label='MSE', color='#ff7f0e')\n",
    "bars_r2 = plt.bar(x + bar_width, results_df['R2'], bar_width, label='R2', color='#2ca02c')\n",
    "\n",
    "# Add actual values on top of each bar (keep this)\n",
    "for bars, metric in zip([bars_rmse, bars_mse, bars_r2], metrics):\n",
    "    for bar, model in zip(bars, models_list):\n",
    "        height = bar.get_height()\n",
    "        actual_value = results_df.loc[model, metric]\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, height, f'{actual_value:.4f}',\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Customize plot (keep this)\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Metric Value\") # Changed y-label back to \"Metric Value\"\n",
    "plt.title(\"Metrics Comparison Across All Models (Original Scale)\") # Changed title\n",
    "plt.xticks(x, models_list, rotation=0) # Keep horizontal labels\n",
    "plt.legend(loc='upper left') # Keep legend position\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"metrics_comparison_original_scale.png\", format=\"png\") # Changed to png\n",
    "plt.close()\n",
    "\n",
    "# Make predictions on the test set and create a DataFrame for comparison\n",
    "predictions_df = pd.DataFrame({'Actual Rating': y_test})\n",
    "\n",
    "for name, model in models.items():\n",
    "    predictions_df[f'Predicted ({name})'] = model.predict(X_test)\n",
    "\n",
    "# Display the predictions DataFrame\n",
    "print(\"\\nPredictions on Test Set (including actual ratings):\\n\")\n",
    "print(predictions_df.head(20)) # Displaying the first 20 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb2012-d5de-4300-8ca6-3640f54e3bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee503e0-22b1-421c-aad4-6348f35ebc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
