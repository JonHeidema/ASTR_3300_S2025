{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e339ae-8f68-4e70-9d54-ebc812bf6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A portion of this code was already written. I had written it for a different project I have been working on, \n",
    "#however the gpr is for this project specifically. Starting from def fit_gp_model to def_main is the additions to the code I already had. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "import logging\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class LightCurveGPR:\n",
    "    def __init__(self, root_directory):\n",
    "        self.root_directory = root_directory\n",
    "    \n",
    "    def load_data_from_sector(self, obj_name, sector, cam, ccd):\n",
    "        directory = f'{self.root_directory}/sector{sector}/cam{cam}_ccd{ccd}/lc_hyperleda'\n",
    "        file_path = os.path.join(directory, f\"lc_{obj_name}_cleaned\")\n",
    "        try:\n",
    "            if os.path.exists(file_path):\n",
    "                data = pd.read_csv(file_path, sep=r'\\s+')\n",
    "                data.columns = data.columns.str.strip()\n",
    "                data = data.replace('-', pd.NA).astype(float)\n",
    "                data['sector'] = int(sector)\n",
    "                return data\n",
    "            else:\n",
    "                logging.debug(f\"File not found: {file_path}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while reading {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def sigma_clip_data(self, data, sigma=3, maxiters=5):\n",
    "        try:\n",
    "            if data is None or data.empty:\n",
    "                return None\n",
    "            data_values = data['cts'].values\n",
    "            mask = np.ones(len(data_values), dtype=bool)\n",
    "            for _ in range(maxiters):\n",
    "                mean = np.mean(data_values[mask])\n",
    "                std = np.std(data_values[mask])\n",
    "                new_mask = np.abs(data_values - mean) < sigma * std\n",
    "                if np.all(new_mask == mask):\n",
    "                    break\n",
    "                mask = new_mask\n",
    "            clipped_data = data.iloc[mask]\n",
    "            return clipped_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during sigma clipping: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def combine_sector_data(self, obj_name):\n",
    "        combined_data = []\n",
    "        sectors = ['02', '03', '04', '05', '07', '19', '20', '21']\n",
    "        for sector in sectors:\n",
    "            for cam in range(1, 5):\n",
    "                for ccd in range(1, 5):\n",
    "                    data = self.load_data_from_sector(obj_name, sector, cam, ccd)\n",
    "                    if data is not None and not data.empty:\n",
    "                        clipped_data = self.sigma_clip_data(data)\n",
    "                        if clipped_data is not None and not clipped_data.empty:\n",
    "                            combined_data.append(clipped_data)\n",
    "                           \n",
    "        if not combined_data:\n",
    "            logging.warning(f\"No data found for {obj_name} in any sector\")\n",
    "            return None\n",
    "        combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "        combined_df = combined_df.sort_values('BTJD')\n",
    "        return combined_df\n",
    "    \n",
    "    def fit_gp_model(self, data, n_restarts=10):\n",
    "        X = data['BTJD'].values.reshape(-1, 1)\n",
    "        y = data['cts'].values\n",
    "        y_err = data['e_cts'].values\n",
    "\n",
    "        self.X_mean = X.mean()\n",
    "        self.X_std = X.std()\n",
    "        self.y_mean = y.mean()\n",
    "        self.y_std = y.std()\n",
    "        X_norm = (X - self.X_mean) / self.X_std\n",
    "        y_norm = (y - self.y_mean) / self.y_std\n",
    "        y_err_norm = y_err / self.y_std\n",
    "\n",
    "        amplitude = ConstantKernel(constant_value=1.0, constant_value_bounds=(0.1, 10.0))\n",
    "        length_scale = 10.0\n",
    "        drw_kernel = amplitude * Matern(length_scale=length_scale, length_scale_bounds=(0.1, 100.0), nu=0.5)\n",
    "        noise_kernel = WhiteKernel(noise_level=np.mean(y_err_norm**2),\n",
    "                                  noise_level_bounds=(np.min(y_err_norm**2), np.max(y_err_norm**2)*10))\n",
    "        kernel = drw_kernel + noise_kernel\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, alpha=y_err_norm**2,\n",
    "                                     n_restarts_optimizer=n_restarts, normalize_y=False)\n",
    "        gp.fit(X_norm, y_norm)\n",
    "        logging.info(f\"Optimized kernel: {gp.kernel_}\")\n",
    "        return gp, X_norm, y_norm, y_err_norm\n",
    "    \n",
    "    def extract_kernel_parameters(self, gp_model):\n",
    "        kernel_params = gp_model.kernel_.get_params()\n",
    "\n",
    "        amplitude = None\n",
    "        length_scale = None\n",
    "        noise_level = None\n",
    "\n",
    "        for param_name, param_value in kernel_params.items():\n",
    "            if 'constant_value' in param_name and not 'bounds' in param_name:\n",
    "                amplitude = param_value\n",
    "            elif 'length_scale' in param_name and not 'bounds' in param_name:\n",
    "                length_scale = param_value\n",
    "            elif 'noise_level' in param_name and not 'bounds' in param_name:\n",
    "                noise_level = param_value\n",
    "                \n",
    "            if isinstance(length_scale, (list, tuple, np.ndarray)):\n",
    "                tau_drw_days = length_scale[0] * self.X_std\n",
    "\n",
    "            else:\n",
    "                tau_drw_days = length_scale * self.X_std\n",
    "            \n",
    "        else:\n",
    "            tau_drw_days = None\n",
    "\n",
    "\n",
    "            if isinstance(amplitude, (list, tuple, np.ndarray)):\n",
    "                amplitude_counts = amplitude[0] * self.y_std\n",
    "                \n",
    "            else:\n",
    "                amplitude_counts = amplitude * self.y_std        \n",
    "            \n",
    "        return {\n",
    "            'amplitude': amplitude_counts,\n",
    "            'tau_drw_days': tau_drw_days,\n",
    "            'noise_level': noise_level\n",
    "        }\n",
    "    \n",
    "    def calculate_statistics(self, data, gp_model, X_norm, y_norm):\n",
    "        X = X_norm * self.X_std + self.X_mean\n",
    "        y = y_norm * self.y_std + self.y_mean\n",
    "        time_span = X.max() - X.min()\n",
    "        mean_flux = np.mean(y)\n",
    "        std_flux = np.std(y)\n",
    "        min_flux = np.min(y)\n",
    "        max_flux = np.max(y)\n",
    "        amplitude = max_flux - min_flux\n",
    "        rms = np.sqrt(np.mean(np.square(y - mean_flux)))\n",
    "        mean_err = np.mean(data['e_cts'].values**2)\n",
    "        snr = mean_flux / mean_err if mean_err > 0 else np.nan\n",
    "        variance = np.var(y)\n",
    "        mean_err_squared = np.mean(data['e_cts'].values**2)\n",
    "        excess_variance = variance - mean_err_squared\n",
    "        \n",
    "        if mean_flux != 0 and excess_variance > 0:\n",
    "            frac_var = np.sqrt(excess_variance) / np.abs(mean_flux)\n",
    "            frac_var_err = np.sqrt(\n",
    "                (1/(2*len(y)*frac_var)) * \n",
    "                (mean_err_squared/mean_flux**2) + \n",
    "                (2*mean_err_squared**2/(len(y)*mean_flux**4))\n",
    "            )\n",
    "        else:\n",
    "            frac_var = np.nan\n",
    "            frac_var_err = np.nan\n",
    "        y_pred_norm = gp_model.predict(X_norm)\n",
    "        y_pred = y_pred_norm * self.y_std + self.y_mean\n",
    "\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        log_likelihood = gp_model.log_marginal_likelihood_value_\n",
    "        \n",
    "        residuals = y - y_pred\n",
    "        errors = data['e_cts'].values\n",
    "        chi2 = np.sum((residuals / errors) **2)\n",
    "        reduced_chi2 = chi2 / (len(y) - 2)\n",
    "        kernel_params = self.extract_kernel_parameters(gp_model)\n",
    "        stats = {\n",
    "            'time_span': time_span,\n",
    "            'mean_flux': mean_flux,\n",
    "            'std_flux': std_flux,\n",
    "            'min_flux': min_flux,\n",
    "            'max_flux': max_flux,\n",
    "            'amplitude': amplitude,\n",
    "            'rms': rms,\n",
    "            'snr': snr,\n",
    "            'variance': variance,\n",
    "            'excess_variance': excess_variance,\n",
    "            'frac_var': frac_var,\n",
    "            'frac_var_err': frac_var_err,\n",
    "            'r2': r2,\n",
    "            'log_likelihood': log_likelihood,\n",
    "            'chi2': chi2,\n",
    "            'reduced_chi2': reduced_chi2,\n",
    "            'kernel_params': kernel_params\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def predict_gp(self, gp, X_norm, X_pred=None, n_samples=20):\n",
    "        if X_pred is None:\n",
    "            X_min, X_max = X_norm.min(), X_norm.max()\n",
    "            X_pred_norm = np.linspace(X_min, X_max, 1000).reshape(-1, 1)\n",
    "        else:\n",
    "            X_pred_norm = (X_pred - self.X_mean) / self.X_std\n",
    "        y_pred_norm, sigma_norm = gp.predict(X_pred_norm, return_std=True)\n",
    "\n",
    "        X_pred = X_pred_norm * self.X_std + self.X_mean\n",
    "        y_pred = y_pred_norm * self.y_std + self.y_mean\n",
    "        sigma = sigma_norm * self.y_std\n",
    "\n",
    "        samples_norm = gp.sample_y(X_pred_norm, n_samples=n_samples)\n",
    "        samples = samples_norm * self.y_std + self.y_mean\n",
    "        \n",
    "        return X_pred, y_pred, sigma, samples\n",
    "    \n",
    "    def plot_gp_results(self, data, X_pred, y_pred, sigma, samples, obj_name, stats, save_dir=None):\n",
    "        plt.figure(figsize=(14, 8))\n",
    "\n",
    "        plt.errorbar(data['BTJD'], data['cts'], yerr=data['e_cts'],\n",
    "                    fmt='o', color='black', ecolor='lightgray',\n",
    "                    elinewidth=1, capsize=2, markersize=4, alpha=0.5, label='Observations')\n",
    "        plt.plot(X_pred, y_pred, 'r-', linewidth=2.5, label='GP Mean')\n",
    "        plt.fill_between(X_pred.flatten(),\n",
    "                        y_pred - 2*sigma,\n",
    "                        y_pred + 2*sigma,\n",
    "                        color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "        for i in range(min(5, samples.shape[1])):\n",
    "            plt.plot(X_pred, samples[:, i], 'r-', alpha=0.1)\n",
    "        sectors = data['sector'].unique()\n",
    "        sectors.sort()\n",
    "        for sector in sectors:\n",
    "            sector_data = data[data['sector'] == sector]\n",
    "            mid_point = len(sector_data) // 2\n",
    "            if not sector_data.empty and mid_point < len(sector_data):\n",
    "                x_pos = sector_data.iloc[mid_point]['BTJD']\n",
    "                y_max = sector_data['cts'].max()\n",
    "                plt.text(x_pos, y_max * 1.05, f\"S{sector}\",\n",
    "                        horizontalalignment='center', fontsize=8)\n",
    "        \n",
    "        plt.title(f\"Gaussian Process Regression for {obj_name}\", fontsize=14)\n",
    "        plt.xlabel('BTJD (Barycentric TESS Julian Date)', fontsize=12)\n",
    "        plt.ylabel('Counts (cts)', fontsize=12)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        stats_text = (\n",
    "            f\"Characteristic timescale: {stats['GP Model Statistics']['Characteristic timescale (days)']:.2f} days\\n\"\n",
    "            f\"Amplitude: {stats['GP Model Statistics']['Amplitude (counts)']:.2f} counts\\n\"\n",
    "            f\"Fractional variability: {stats['Data Statistics']['Fractional variability']:.4f}\\n\"\n",
    "            f\"R-squared: {stats['GP Model Statistics']['R-squared']:.4f}\\n\"\n",
    "            f\"RMS: {stats['Data Statistics']['RMS']:.4f}\\n\"\n",
    "            f\"SNR: {stats['Data Statistics']['SNR']:.4f}\\n\"\n",
    "            f\"Reduced χ²: {stats['GP Model Statistics']['Reduced Chi-squared']:.4f}\"\n",
    "        )\n",
    "        plt.figtext(0.15, 0.85, stats_text, \n",
    "                   bbox=dict(facecolor='white', alpha=0.8, boxstyle='round'),\n",
    "                   fontsize=10)\n",
    "        plt.figtext(0.5, 0.01, f\"Sectors: {', '.join([f'S{s}' for s in sectors])}\",\n",
    "                  ha=\"center\", fontsize=8, bbox={\"facecolor\":\"white\", \"alpha\":0.5, \"pad\":5})\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.97]) \n",
    "        \n",
    "        if save_dir:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(save_dir, f\"{obj_name}_gp_regression.png\")\n",
    "            plt.savefig(save_path, dpi=300)\n",
    "            logging.info(f\"GP regression plot saved to {save_path}\")\n",
    "              \n",
    "        plt.show()\n",
    "\n",
    "    def run_full_analysis(self, obj_name, save_dir=None):\n",
    "        logging.info(f\"Starting analysis for {obj_name}\")\n",
    "        data = self.combine_sector_data(obj_name)\n",
    "        \n",
    "        if data is None or data.empty:\n",
    "            logging.error(f\"No data found for {obj_name}\")\n",
    "            return None\n",
    "        gp_model, X_norm, y_norm, y_err_norm = self.fit_gp_model(data)\n",
    "        stats = self.calculate_statistics(data, gp_model, X_norm, y_norm)\n",
    "        X_pred, y_pred, sigma, samples = self.predict_gp(gp_model, X_norm)\n",
    "        self.plot_gp_results(data, X_pred, y_pred, sigma, samples, obj_name, stats, save_dir)\n",
    "        \n",
    "        return {\n",
    "            'data': data,\n",
    "            'gp_model': gp_model,\n",
    "            'X_norm': X_norm,\n",
    "            'y_norm': y_norm,\n",
    "            'statistics': stats,\n",
    "            'predictions': {\n",
    "                'X_pred': X_pred,\n",
    "                'y_pred': y_pred,\n",
    "                'sigma': sigma,\n",
    "                'samples': samples\n",
    "            }\n",
    "        }\n",
    "\n",
    "def read_object_list(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return object\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading object list from {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    root_directory = '/home/kicowlin/SummerResearch2024'\n",
    "    save_directory = f'{root_directory}/gp_regression'\n",
    "    obj_list_file = f'{root_directory}/object_list.txt'\n",
    "    gpr = LightCurveGPR(root_directory)\n",
    "    obj_list = read_object_list(obj_list_file) \n",
    "\n",
    "    for obj_name in obj_list:\n",
    "        logging.info(f\"Processing {obj_name}\")\n",
    "        gpr.run_full_analysis(obj_name, save_dir=save_directory)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:astr3300] *",
   "language": "python",
   "name": "conda-env-astr3300-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
